{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rk-9UZY_xe2"
      },
      "source": [
        "# UMC 301: Applied Data Science and Artificial Intelligence\n",
        "## Assignment 2\n",
        "\n",
        "### Submission instructions:\n",
        "\n",
        "1.   The assignment is to be submitted in ONE single notebook.\n",
        "2.   Submit the .ipynb file and pdf of the same with all cells open through this Teams Assignment.\n",
        "3. If your IISc email ID is < username > @iisc.ac.in, then name the file < username >_Assgn_2. E.g. jonathan_Assgn_1a for email ID jonathan@iisc.ac.in.\n",
        "4. Before submission, execute the ’Restart session and run all’ option from the Runtime/Kernel tab. Verify that there are no errors and that you are getting the output you expect.\n",
        "5. Use the dataset : https://www.dropbox.com/scl/fi/7m0pt1dkmwbkr3byv5r2j/face_images.zip?rlkey=r2gqsdvuyvpqqk4a7bjd9kcvv&st=nj7wl35d&dl=1\n",
        "**Data Files**\n",
        "\n",
        "**face_images** : All the face images, cropped and aligned. <br>\n",
        "**face_image_bbox.csv**: Bounding box information for each image. \"x_1\" and \"y_1\" represent the upper left point coordinate of bounding box. \"width\" and \"height\" represent the width and height of bounding box. <br>\n",
        "**face_image_attr.csv**: Attribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bma59Wpv_xe3"
      },
      "source": [
        "# Question 1\n",
        "Train a ResNet18 model on the above dataset to classify the \"Arched_Eyebrows\" attribute. Apply appropriate data pre-processing [you can use less datapoints]. Evaluate the model by calculating precision, accuracy, F1 score, ROC curve, confusion matrix, and other relevant metrics. Do experiments with different hyperparameters and discuss about results. <br>\n",
        "[Note: Marks will be given based on different experiments and discussion]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3HgsAYTVTn2g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XWwtNqNGTn2h"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v2wd4rBgTn2h"
      },
      "outputs": [],
      "source": [
        "# Load an image\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    if max_size:\n",
        "        scale = max_size / max(image.size)\n",
        "        size = image.size[0]*scale, image.size[1]*scale\n",
        "        image = image.resize(size)\n",
        "\n",
        "    if shape:\n",
        "        image = image.resize(shape)\n",
        "\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define a custom Dataset Class by extending the Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = pd.read_csv('face_images/face_image_eval_partition.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.loc[:60000, 'partition'] = 0\n",
        "x.loc[60000:110000, 'partition'] = 1\n",
        "x.loc[110000:,'partition'] =2\n",
        "x = x.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.to_csv('face_images/custom_partition.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s4epfRt9Tn2h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file_split, csv_attr, transform=None, split=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file containing image paths, splits, and labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            split (str): Which dataset split to load ('train', 'test', 'val').\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_file_split)\n",
        "        self.data = self.data[self.data['partition'] == split]  # Filter by the desired split\n",
        "        self.transform = transform\n",
        "        self.eyebrows_arched = pd.read_csv(csv_attr)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = 'face_images/face_images/img_align_celeba/'+self.data.iloc[idx, 0]\n",
        "        imgid = self.data.iloc[idx, 0]\n",
        "        label = self.eyebrows_arched[self.eyebrows_arched['image_id'] == imgid]['Arched_Eyebrows'].values[0]\n",
        "        image = Image.open(img_path).convert('RGB')  # Load image\n",
        "        if label == -1:\n",
        "            label = 0\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define out transformer and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oIFD6cEMTn2i"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "opuCTcSZTn2i"
      },
      "outputs": [],
      "source": [
        "Train = CustomDataset(csv_file_split='face_images/custom_partition.csv', csv_attr='face_images/face_image_attr.csv', transform=transform, split=0)\n",
        "Val = CustomDataset(csv_file_split='face_images/custom_partition.csv', csv_attr='face_images/face_image_attr.csv', transform=transform, split=1)\n",
        "Test = CustomDataset(csv_file_split='face_images/custom_partition.csv', csv_attr='face_images/face_image_attr.csv', transform=transform, split=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>partition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000003.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000005.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>059996.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>059997.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>059998.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>059999.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>060000.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         image_id  partition\n",
              "0      000001.jpg          0\n",
              "1      000002.jpg          0\n",
              "2      000003.jpg          0\n",
              "3      000004.jpg          0\n",
              "4      000005.jpg          0\n",
              "...           ...        ...\n",
              "59995  059996.jpg          0\n",
              "59996  059997.jpg          0\n",
              "59997  059998.jpg          0\n",
              "59998  059999.jpg          0\n",
              "59999  060000.jpg          0\n",
              "\n",
              "[60000 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Train.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JQMj5bfpTn2i"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "test_loader = DataLoader(Test, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next, load pretrained weights and remove the default fully connected layer and add in the fully connected layer that we require"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uygRJZwfTn2i",
        "outputId": "c58e15a1-d598-4436-e969-5f956364a9de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/club/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/club/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final layer to match binary classification\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1),  # Single output for binary classification\n",
        "    nn.Sigmoid()  # Sigmoid activation function\n",
        ")\n",
        "resnet18 = resnet18.cuda()  # Move model to GPU if available\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Rlle_j6qTn2j"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "optimizers = {\n",
        "    'Adam': optim.Adam,\n",
        "    'SGD' : optim.SGD,\n",
        "    'AdamW' : optim.AdamW\n",
        "}\n",
        "\n",
        "learning_rates = [1e-4, 1e-3, 5e-4]\n",
        "batch_sizes = [16,32,64]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This is our training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MO-TBVfRTn2j",
        "outputId": "cbcbdfa9-b8a3-456e-8a6e-a7463462b8c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 1533/3750 [03:55<05:41,  6.50it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[17], line 27\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_images/face_images/img_align_celeba/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m imgid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meyebrows_arched[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meyebrows_arched\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimgid\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArched_Eyebrows\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop (simplified)\n",
        "for opt in optimizers:\n",
        "    results[opt] = dict()\n",
        "    for lr in learning_rates:\n",
        "        results[opt][lr] = dict()\n",
        "        for bs in batch_sizes:\n",
        "            results[opt][lr][bs] = {\n",
        "                'train_loss': [],  # Store training loss for each epoch\n",
        "                'val_loss': [],    # Store validation loss for each epoch\n",
        "                'best_val_loss': float('inf'),  # Track the best validation loss\n",
        "                'best_weights': None  # Store the best model weights\n",
        "            }\n",
        "\n",
        "            num_epochs = 20\n",
        "            optimizer = optimizers[opt](resnet18.parameters(), lr=lr)\n",
        "\n",
        "            train_loader = DataLoader(Train, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(Val, batch_size=bs, shuffle=False)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                resnet18.train()\n",
        "                running_loss = 0.0\n",
        "\n",
        "                # Training phase\n",
        "                for images, labels in tqdm(train_loader, total=len(train_loader)):\n",
        "                    images, labels = images.cuda(), labels.float().cuda()\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = resnet18(images)\n",
        "                    loss = criterion(outputs, labels.unsqueeze(1))  # Ensure correct shape for BCE loss\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                # Compute and store average training loss for this epoch\n",
        "                avg_train_loss = running_loss / len(train_loader)\n",
        "                results[opt][lr][bs]['train_loss'].append(avg_train_loss)\n",
        "\n",
        "                # Validation phase\n",
        "                resnet18.eval()\n",
        "                val_running_loss = 0.0\n",
        "\n",
        "                with torch.no_grad():  # No gradient computation in evaluation mode\n",
        "                    for images, labels in tqdm(val_loader, total=len(val_loader), colour='red'):\n",
        "                        images, labels = images.cuda(), labels.float().cuda()\n",
        "                        outputs = resnet18(images)\n",
        "                        val_loss = criterion(outputs, labels.unsqueeze(1))  # Validation loss\n",
        "                        val_running_loss += val_loss.item()\n",
        "\n",
        "                # Compute and store average validation loss for this epoch\n",
        "                avg_val_loss = val_running_loss / len(val_loader)\n",
        "                results[opt][lr][bs]['val_loss'].append(avg_val_loss)\n",
        "\n",
        "                # Check if this is the best validation loss so far\n",
        "                if avg_val_loss < results[opt][lr][bs]['best_val_loss']:\n",
        "                    results[opt][lr][bs]['best_val_loss'] = avg_val_loss\n",
        "                    results[opt][lr][bs]['best_weights'] = resnet18.state_dict().copy()  # Store best weights\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss} , lr={lr}, bs={bs}, opt={opt}\")\n",
        "\n",
        "            \n",
        "            # Optionally save the best model weights to a separate file\n",
        "            torch.save(results[opt][lr][bs]['best_weights'], \n",
        "                       f'./output/best_weights-opt_{opt}-lr_{lr}-bs_{bs}.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': resnet18.state_dict(),\n",
        "    'criterion_state_dict': criterion.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()}, './output/resnet18-checkpoint.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(resnet18, './output/resnet18-model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Used to resume the work by loading a presaved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = torch.load('./output/resnet18-model.pt', weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = model2.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next, let's get the relevent metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model2.eval()\n",
        "\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, total=len(test_loader), colour='green'):  # Use test_loader here\n",
        "        inputs = inputs.cuda()\n",
        "        labels_np = labels.numpy()\n",
        "        outputs = model2(inputs)\n",
        "        preds = 0\n",
        "        if outputs>0.5:\n",
        "            preds = [1]\n",
        "        else:\n",
        "            preds = [0]\n",
        "        \n",
        "        #print(f'Label: {labels_np}, Prediction: {preds}, Probability: {outputs.cpu().numpy()}')\n",
        "        all_labels.extend(labels_np)\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_probs.extend(outputs.cpu().numpy()[0])\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Not Arched', 'Arched'], \n",
        "            yticklabels=['Not Arched', 'Arched'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix on Test Set')\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1 Score: {f1:.4f}')\n",
        "\n",
        "# ROC Curve\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) on Test Set')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pr_curve = precision_recall_curve(all_labels, all_probs)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(pr_curve[1], pr_curve[0], color='darkorange', lw=2, \n",
        "         label=f'PR curve')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve on Test Set')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Ox51FG_xe3"
      },
      "source": [
        "# Question 2\n",
        "Capture a photo of yourself, similar to the dataset mentioned above, and *visualize the activation maps* and *filters* at various layers of the model trained in Question 1. Then, conduct an occlusion experiment to examine the impact on the model's predictions. [I hope you won't copy this 😀]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load my photo\n",
        "myphoto = load_image('sirjanh.jpg', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(myphoto[0].permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [],
      "source": [
        "myphoto2=myphoto.cuda()\n",
        "model2 = model2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    output = model2(myphoto2)\n",
        "    print(output)\n",
        "    if output>0.5:\n",
        "        print('Arched')\n",
        "    else:\n",
        "        print('Not Arched')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {},
      "outputs": [],
      "source": [
        "activations=[]\n",
        "\n",
        "def hook_fn(module, input, output):\n",
        "    activations.append(output)\n",
        "\n",
        "hooks = []\n",
        "for name, module in model2.named_modules():\n",
        "    if type(module) == nn.Conv2d:\n",
        "        hook = module.register_forward_hook(hook_fn)\n",
        "        hooks.append(hook)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model2(myphoto2)\n",
        "\n",
        "for hk in hooks:\n",
        "    hk.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "def viz_activation_conv(activations, max_columns=8):\n",
        "    for lay in activations:\n",
        "        new_acti = torch.squeeze(lay)\n",
        "        \n",
        "        if len(new_acti.shape) == 3:\n",
        "            num_maps, height, width = new_acti.shape\n",
        "        elif len(new_acti.shape) == 4:\n",
        "            num_maps, height, width = new_acti.shape[1:]\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected shape for activation layer\")\n",
        "        \n",
        "        # Calculate adaptive figure size\n",
        "        aspect_ratio = width / height\n",
        "        fig_width = min(20, 2 * max_columns)\n",
        "        fig_height = fig_width / aspect_ratio / (max_columns / math.ceil(num_maps / max_columns))\n",
        "        \n",
        "        # Calculate number of rows and columns\n",
        "        num_columns = min(num_maps, max_columns)\n",
        "        num_rows = math.ceil(num_maps / num_columns)\n",
        "        \n",
        "        print(f\"Plotting {num_maps} maps in {num_rows} rows and {num_columns} columns\")\n",
        "        \n",
        "        fig = plt.figure(figsize=(fig_width, fig_height))\n",
        "        gs = fig.add_gridspec(num_rows, num_columns)\n",
        "        \n",
        "        for i in range(num_maps):\n",
        "            ax = fig.add_subplot(gs[i // num_columns, i % num_columns])\n",
        "            if len(new_acti.shape) == 3:\n",
        "                im = ax.imshow(new_acti[i].cpu().numpy(), cmap='viridis', aspect='auto')\n",
        "            else:\n",
        "                im = ax.imshow(new_acti[0, i].cpu().numpy(), cmap='viridis', aspect='auto')\n",
        "            ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout(pad=0.5)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "viz_activation_conv(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def plot_conv_filters(model, figsize=(12, 12), max_filters=64, max_subplots=64):\n",
        "    conv_layers = [module for module in model.modules() if isinstance(module, torch.nn.Conv2d)]\n",
        "    \n",
        "    for idx, conv_layer in enumerate(conv_layers):\n",
        "        weights = conv_layer.weight.data.cpu().numpy()\n",
        "        num_filters, num_channels, height, width = weights.shape\n",
        "        \n",
        "        # Limit the number of filters to plot\n",
        "        num_filters_to_plot = num_filters\n",
        "        \n",
        "        # Calculate grid size\n",
        "        grid_size = min(math.ceil(math.sqrt(num_filters_to_plot)), math.ceil(math.sqrt(max_subplots)))\n",
        "        \n",
        "        fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)\n",
        "        fig.suptitle(f'Filters in Convolutional Layer {idx+1}', fontsize=16)\n",
        "        \n",
        "        for i in range(grid_size * grid_size):\n",
        "            ax = axes[i // grid_size, i % grid_size]\n",
        "            \n",
        "            if i < num_filters_to_plot:\n",
        "                # For filters with multiple input channels, we'll visualize the first channel\n",
        "                filter_img = weights[i, 0]\n",
        "                \n",
        "                im = ax.imshow(filter_img)\n",
        "                ax.axis('off')\n",
        "            else:\n",
        "                ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(top=0.9)  # Adjust for the suptitle\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_conv_filters(model2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Occlusion Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2(myphoto2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = myphoto2.clone().cpu()\n",
        "model_req = model2.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "1-model_req(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [],
      "source": [
        "def occlusion(model, image, label, occ_size = 50, occ_stride = 6, occ_pixel = 0.4):\n",
        "\n",
        "    #get the width and height of the image\n",
        "    width, height = image.shape[-2], image.shape[-1]\n",
        "    print(image.shape)\n",
        "    #setting the output image width and height\n",
        "    output_height = int(np.ceil((height)/occ_stride))\n",
        "    output_width = int(np.ceil((width)/occ_stride))\n",
        "\n",
        "    #create a white image of sizes we defined\n",
        "    heatmap = torch.zeros((output_height, output_width))\n",
        "\n",
        "    #iterate all the pixels in each column\n",
        "    for h in tqdm(range(0, height, occ_stride), desc='Progress', leave=True, colour='green'):\n",
        "        for w in tqdm(range(0, width, occ_stride), desc='Progress', leave=True, colour='red'):\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "            # SLIDE ACROSS INPUT\n",
        "            w_start, w_end = w, min(width, w+occ_size)\n",
        "            h_start, h_end = h, min(height, h+int(occ_size//8))\n",
        "\n",
        "            input_image = image.clone().detach()\n",
        "\n",
        "            #replacing all the pixel information in the image with occ_pixel(grey) in the specified location\n",
        "            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n",
        "\n",
        "            #run inference on modified image\n",
        "            output = model(input_image)\n",
        "            prob = (1-label)-output.tolist()[0][0]\n",
        "\n",
        "            #setting the heatmap location to probability value\n",
        "            heatmap[h//occ_stride, w//occ_stride] = prob \n",
        "\n",
        "    return heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = model2.cuda()\n",
        "pic_req = myphoto2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "heatmap = occlusion(temp, pic_req, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False)\n",
        "figure = imgplot.get_figure() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {},
      "outputs": [],
      "source": [
        "def overlay_heatmap_on_image(image, heatmap, alpha=0.5, colormap='hot'):\n",
        "    \"\"\"\n",
        "    Overlays the heatmap on the original image.\n",
        "    \n",
        "    Args:\n",
        "    - image (torch.Tensor): The original image tensor in shape (1, C, H, W)\n",
        "    - heatmap (torch.Tensor): The heatmap generated from the occlusion experiment\n",
        "    - alpha (float): The transparency level for the heatmap overlay\n",
        "    - colormap (str): The colormap to apply to the heatmap\n",
        "    \n",
        "    Returns:\n",
        "    - None (Displays the image with the heatmap overlay)\n",
        "    \"\"\"\n",
        "    # Convert the image and heatmap to NumPy arrays for plotting\n",
        "    image_np = image.squeeze().permute(1, 2, 0).cpu().numpy()  # Convert from CHW to HWC format\n",
        "    image_np = np.clip(image_np, 0, 1)  # Ensure image is in valid range (0, 1)\n",
        "\n",
        "    # Normalize the heatmap to (0, 1)\n",
        "    heatmap_np = heatmap.cpu().numpy()\n",
        "    heatmap_np = (heatmap_np - heatmap_np.min()) / (heatmap_np.max() - heatmap_np.min())  # Normalize heatmap\n",
        "\n",
        "    # Resize heatmap to match the image size (using bilinear interpolation)\n",
        "    heatmap_resized = torch.nn.functional.interpolate(\n",
        "        torch.tensor(heatmap_np).unsqueeze(0).unsqueeze(0),\n",
        "        size=(image_np.shape[0], image_np.shape[1]),\n",
        "        mode='bilinear'\n",
        "    ).squeeze().numpy()\n",
        "\n",
        "    # Apply a colormap to the heatmap\n",
        "    heatmap_colored = cm.get_cmap(colormap)(heatmap_resized)\n",
        "    heatmap_colored = np.delete(heatmap_colored, 3, 2)  # Remove the alpha channel from the colormap\n",
        "\n",
        "    # Blend the original image and the heatmap\n",
        "    blended_image = (1 - alpha) * image_np + alpha * heatmap_colored\n",
        "\n",
        "    # Plot the original image, heatmap, and blended result\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Display the original image with a grid\n",
        "    ax = plt.subplot(1, 3, 1)\n",
        "    ax.imshow(image_np)\n",
        "    ax.set_title('Original Image with Grid')\n",
        "\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(heatmap_resized, cmap=colormap)\n",
        "    plt.title('Occlusion Heatmap')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(blended_image)\n",
        "    plt.title('Image with Heatmap Overlay')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "heatmap.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overlay_heatmap_on_image(pic_req, heatmap)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
