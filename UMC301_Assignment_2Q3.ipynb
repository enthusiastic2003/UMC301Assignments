{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad81b4f0-6852-4ecf-a576-c19850a5bdc3",
   "metadata": {},
   "source": [
    " # Question 3\n",
    " Train a YOLOv3 model on the dataset mentioned above for object localization and evaluate its performance. Provide model output from data point used in Question 2. <br>\n",
    " [Note: Marks will be given based on different experiments and discussion]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dda0028-c387-4212-b1c2-32741560d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38289171-94b3-4c60-8204-f8a4272ec6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Block: Conv -> BatchNorm -> LeakyReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.leaky(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9339601b-8660-42dd-946d-f04c0d4bc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block: (Conv -> Conv) + Skip Connection\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First 1x1 convolution to reduce channels\n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        # Then 3x3 convolution to restore channels\n",
    "        self.conv2 = ConvBlock(out_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        return out + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7e81da-0fed-468d-8932-0f723d942cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, scale_factor=2, mode = 'nearest'):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode = mode)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953d1c80-bd6a-4b13-9290-707987cf0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Darknet53,  self).__init__()\n",
    "        self.layers = self.__create_layers__()\n",
    "    \n",
    "    def __create_layers__(self):\n",
    "        layers = []\n",
    "        in_chan = 3\n",
    "        layers.append(ConvBlock(in_chan, 32, 3, 1, 1))\n",
    "\n",
    "        in_chan = 32\n",
    "\n",
    "        res_blocks = [1,2,8,8,4]\n",
    "\n",
    "        conv_blocks_filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        self.pause = []\n",
    "        for i, chn_sz in enumerate(conv_blocks_filters):\n",
    "            layers.append(ConvBlock(in_chan, chn_sz, 3, 2, 1))\n",
    "            in_chan = chn_sz\n",
    "            for b in range(res_blocks[i]):\n",
    "                layers.append(ResidualBlock(in_chan, in_chan//2))\n",
    "            \n",
    "            if i == 2 or i==3 or i==4:\n",
    "                self.pause.append(len(layers)-1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        outputs = []\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if layer_num in self.pause:\n",
    "                outputs.append(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a91896-5b95-432e-8314-ee4dad6cd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class DetectorHead(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_classes):\n",
    "        super(DetectorHead, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(out_channels, in_channels * 2, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels * 2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(out_channels, in_channels * 2, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels * 2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(out_channels, in_channels * 2, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels * 2, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            # Final convolution for predictions\n",
    "            nn.Conv2d(out_channels, 3 * (5 + num_classes), kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10b6206-9285-4376-9bed-c41566052584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes=80):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = Darknet53()\n",
    "        \n",
    "        # Detection Heads\n",
    "        # For different scales: 13x13, 26x26, 52x52\n",
    "        self.head_13 = DetectorHead(in_channels=  1024, out_channels=512, num_classes=num_classes)\n",
    "        self.head_26 = DetectorHead(in_channels=  768, out_channels=256, num_classes=num_classes)\n",
    "        self.head_52 = DetectorHead(in_channels=  384, out_channels=128, num_classes=num_classes)\n",
    "        \n",
    "        # Additional layers for upsampling and concatenation\n",
    "        self.upsample = Upsample()\n",
    "        self.conv_set_26 = nn.Sequential(\n",
    "            ConvBlock(1024, 256, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        self.conv_set_52 = nn.Sequential(\n",
    "            ConvBlock( 768, 128, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock( 128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock( 256, 128, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock( 128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock( 256, 128, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone\n",
    "        features = self.backbone(x)  # [52x52, 26x26, 13x13]\n",
    "        feat_52, feat_26, feat_13 = features\n",
    "        \n",
    "        # Detection Head at 13x13\n",
    "        out_13 = self.head_13(feat_13)\n",
    "        \n",
    "        # Processing for 26x26\n",
    "        feat_26_processed = self.conv_set_26(feat_13)\n",
    "        upsampled_26 = self.upsample(feat_26_processed)\n",
    "        # Concatenate with feat_26 from backbone\n",
    "        concat_26 = torch.cat([upsampled_26, feat_26], dim=1)\n",
    "        # Detection Head at 26x26\n",
    "        out_26 = self.head_26(concat_26)\n",
    "        \n",
    "        # Processing for 52x52\n",
    "        feat_52_processed = self.conv_set_52(concat_26)\n",
    "        upsampled_52 = self.upsample(feat_52_processed)\n",
    "\n",
    "        # Concatenate with feat_52 from backbone\n",
    "        concat_52 = torch.cat([upsampled_52, feat_52], dim=1)\n",
    "        # Detection Head at 52x52\n",
    "        out_52 = self.head_52(concat_52)\n",
    "        \n",
    "        return out_52, out_26, out_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc5f30f-e55f-4dcb-bb15-9a6227f62903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YOLOv3Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes=1, ignore_thresh=0.5, lambda_coord=1, lambda_noobj=0.5):\n",
    "        super(YOLOv3Loss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.ignore_thresh = ignore_thresh\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')  # For localization\n",
    "        self.bce_loss = nn.BCELoss(reduction='sum')  # For objectness and class\n",
    "\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        out_52, out_26, out_13 = predictions\n",
    "        loss = 0\n",
    "\n",
    "        # Define the scales and corresponding anchors\n",
    "        scales = [\n",
    "            {'output': out_52, 'anchors': self.anchors[0], 'stride': 8},\n",
    "            {'output': out_26, 'anchors': self.anchors[1], 'stride': 16},\n",
    "            {'output': out_13, 'anchors': self.anchors[2], 'stride': 32},\n",
    "        ]\n",
    "\n",
    "        for scale in scales:\n",
    "            output = scale['output']\n",
    "            anchors = scale['anchors']\n",
    "            stride = scale['stride']\n",
    "            grid_size = output.size(2)\n",
    "            batch_size = output.size(0)\n",
    "\n",
    "            # Reshape output\n",
    "            prediction = output.view(batch_size, len(anchors), 5 + self.num_classes, grid_size, grid_size)\n",
    "            prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()\n",
    "\n",
    "            # Sigmoid the center_x, center_y, and objectness score\n",
    "            x = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "            y = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "            w = prediction[..., 2]  # Width\n",
    "            h = prediction[..., 3]  # Height\n",
    "            objectness = torch.sigmoid(prediction[..., 4])\n",
    "            class_probs = torch.sigmoid(prediction[..., 5:])  # Since num_classes=1\n",
    "\n",
    "            # Create grid for calculating offsets\n",
    "            grid_x = torch.arange(grid_size, device=x.device).repeat(grid_size, 1).view([1, 1, grid_size, grid_size])\n",
    "            grid_y = torch.arange(grid_size, device=y.device).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size])\n",
    "\n",
    "            # Calculate the actual center positions\n",
    "            pred_boxes = torch.zeros_like(prediction[..., :4])\n",
    "            pred_boxes[..., 0] = (x + grid_x) * stride\n",
    "            pred_boxes[..., 1] = (y + grid_y) * stride\n",
    "\n",
    "            anchors_tensor = torch.tensor(anchors).to(pred_boxes.device)  # Convert anchors to a tensor\n",
    "            pred_boxes[..., 2] = torch.exp(w) * anchors_tensor[..., 0].view(1, -1, 1, 1)  # Width\n",
    "            pred_boxes[..., 3] = torch.exp(h) * anchors_tensor[..., 1].view(1, -1, 1, 1)  # Height\n",
    "\n",
    "            # Initialize no-object mask\n",
    "            noobj_mask = torch.ones_like(objectness)\n",
    "            \n",
    "            # Process targets for each batch\n",
    "            for b in range(batch_size):\n",
    "                if len(targets[b]) == 0:\n",
    "                    continue\n",
    "\n",
    "                target = targets[b]\n",
    "                tx, ty, tw, th = target  # Assuming the target format [x, y, w, h]\n",
    "                \n",
    "                # Calculate grid cell\n",
    "                gx = int(tx // stride)\n",
    "                gy = int(ty // stride)\n",
    "                \n",
    "                # Calculate offsets and ground truth\n",
    "                gx_offset = (tx / stride) - gx\n",
    "                gy_offset = (ty / stride) - gy\n",
    "                gt_w = torch.log(tw / anchors_tensor[..., 0] + 1e-16)\n",
    "                gt_h = torch.log(th / anchors_tensor[..., 1] + 1e-16)\n",
    "\n",
    "                # Localization loss\n",
    "                loss += self.lambda_coord * (\n",
    "                    self.mse_loss(x[b, :, gy, gx], torch.tensor(gx_offset, device=x.device)) +\n",
    "                    self.mse_loss(y[b, :, gy, gx], torch.tensor(gy_offset, device=y.device)) +\n",
    "                    self.mse_loss(w[b, :, gy, gx], torch.tensor(gt_w, device=w.device)) +\n",
    "                    self.mse_loss(h[b, :, gy, gx], torch.tensor(gt_h, device=h.device))\n",
    "                )\n",
    "\n",
    "                # Objectness loss\n",
    "\n",
    "                target_objectness = torch.ones_like(objectness[b, :, gy, gx], device=objectness.device)\n",
    "\n",
    "                loss += self.bce_loss(objectness[b, :, gy, gx], target_objectness)\n",
    "\n",
    "                # Class prediction loss\n",
    "                target_prob = torch.ones_like(class_probs[b, :, gy, gx], device=class_probs.device)\n",
    "                loss += self.bce_loss(class_probs[b, :, gy, gx], target_prob)\n",
    "\n",
    "                # Mark the grid cell as responsible for the object\n",
    "                noobj_mask[b, :, gy, gx] = 0\n",
    "\n",
    "            # No-object loss: For grid cells that don't contain objects\n",
    "            loss += self.lambda_noobj * self.bce_loss(objectness * noobj_mask, torch.zeros_like(objectness))\n",
    "\n",
    "        return loss / (batch_size + 1e-16)  # Normalize by batch size to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae8a910-e021-4655-bbd9-80e0418fff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628789fe-6f1a-4c58-9c50-3ca7b5242bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "anchors = np.array([\n",
    "    [(10,13), (16,30), (33,23)],  # 52x52\n",
    "    [(30,61), (62,45), (59,119)],  # 26x26\n",
    "    [(116,90), (156,198), (373,326)]  # 13x13\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18fe6561-794f-481c-94a0-e08cc2e677d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "criterion = YOLOv3Loss(anchors=anchors, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df995a72-6f61-4c62-91a5-db41908d614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/club/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, part=0, target_size=(416, 416), transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with annotations.\n",
    "            img_dir (str): Directory with all the images.\n",
    "            target_size (tuple): Desired output size of the images (width, height).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        train, validate, test = \\\n",
    "              np.split(self.annotations.sample(frac=1, random_state=22188), \n",
    "                       [int(.6*len(self.annotations)), int(.8*len(self.annotations))])\n",
    "        if type == 0:\n",
    "            self.annotations = train\n",
    "        elif type == 1:\n",
    "            self.annotations = validate\n",
    "        else:\n",
    "            self.annotations = test\n",
    "        \n",
    "        self.img_dir = img_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def resize_image_and_bbox(self, image, bbox):\n",
    "        \"\"\"\n",
    "        Resize image and adjust bounding box according to the new image size.\n",
    "        \"\"\"\n",
    "        original_height, original_width = image.shape[:2]\n",
    "        resized_image = cv2.resize(image, self.target_size)\n",
    "\n",
    "        scale_x = self.target_size[0] / original_width\n",
    "        scale_y = self.target_size[1] / original_height\n",
    "\n",
    "        x0, y0, x1, y1 = bbox\n",
    "        x0 = int(x0 * scale_x)\n",
    "        y0 = int(y0 * scale_y)\n",
    "        x1 = int(x1 * scale_x)\n",
    "        y1 = int(y1 * scale_y)\n",
    "\n",
    "        return resized_image, (x0, y0, x1, y1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image file name and bounding box from the annotations\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = cv2.imread(img_name)\n",
    "\n",
    "        # Get original bounding box\n",
    "        bbox = self.annotations.iloc[idx, 3:7].values\n",
    "        bbox = list(map(int, bbox))  # Convert to integers\n",
    "\n",
    "        # Resize image and adjust the bounding box\n",
    "        image, bbox = self.resize_image_and_bbox(image, bbox)\n",
    "\n",
    "        # Apply any transforms (if provided)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert image and bbox to PyTorch tensors\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()  # Convert to (C, H, W) format\n",
    "        bbox = torch.tensor(bbox).float()\n",
    "\n",
    "        return image, bbox\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Create dataset\n",
    "csv_file = 'yolo_p3/faces.csv'\n",
    "img_dir = 'yolo_p3/images/'\n",
    "train = YOLODataset(csv_file=csv_file, img_dir=img_dir, part =0 )\n",
    "valid = \n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b5096-2724-4385-baec-9f7aa996b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_bbox(image, bbox, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draws a bounding box on the image.\n",
    "    \n",
    "    Args:\n",
    "    - image: The image on which to draw (in numpy format).\n",
    "    - bbox: The bounding box coordinates (x0, y0, x1, y1).\n",
    "    - color: The color of the bounding box (default is green).\n",
    "    - thickness: The thickness of the bounding box lines.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = list(map(int, bbox))\n",
    "    image_with_bbox = cv2.rectangle(image.copy(), (x0, y0), (x1, y1), color, thickness)\n",
    "    return image_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022af2c-d2d0-49c9-8a8e-1773d12b01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "device = \"cuda\"\n",
    "model = YOLOv3(num_classes=1).to(device)  # Single class case\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = YOLOv3Loss(anchors)  # Your custom loss function\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abed89-1cbc-4dc8-9b0f-617f70757d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/210 [00:00<?, ?it/s]<ipython-input-9-2ac2b5d65c37>:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mse_loss(x[b, :, gy, gx], torch.tensor(gx_offset, device=x.device)) +\n",
      "/home/club/miniconda3/envs/tf-gpu/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "<ipython-input-9-2ac2b5d65c37>:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mse_loss(y[b, :, gy, gx], torch.tensor(gy_offset, device=y.device)) +\n",
      "<ipython-input-9-2ac2b5d65c37>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mse_loss(w[b, :, gy, gx], torch.tensor(gt_w, device=w.device)) +\n",
      "<ipython-input-9-2ac2b5d65c37>:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mse_loss(h[b, :, gy, gx], torch.tensor(gt_h, device=h.device))\n",
      "Epoch 1/10:   5%|â–Œ         | 11/210 [00:04<01:27,  2.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mUntitled-1:37\u001b[0m\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update model weights\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Update running loss\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     39\u001b[0m \u001b[39m# Print average loss per epoch\u001b[39;00m\n\u001b[1;32m     40\u001b[0m avg_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume these are already defined:\n",
    "# - YOLOv3: Your YOLO model class\n",
    "# - YOLOLoss: The loss function class\n",
    "# - dataset: Your custom dataset\n",
    "# - DataLoader: To load batches of data\n",
    "\n",
    "# Set up model, optimizer, and loss function\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set your desired number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Loop over data\n",
    "    for images, bboxes in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = images.to(device)  # Move images to the GPU/CPU\n",
    "        bboxes = bboxes.to(device)  # Move bounding boxes to the GPU/CPU\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Compute the loss (assuming loss function handles multiple scales: 13x13, 26x26, 52x52)\n",
    "        loss = criterion(outputs, bboxes)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update model weights\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save the model checkpoint (optional)\n",
    "    torch.save(model.state_dict(), f\"yolov3_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
